"""
Convergence Tracker
ÏàòÎ†¥ Ìå®ÌÑ¥ Î™®ÎãàÌÑ∞ÎßÅ Î∞è Î∂ÑÏÑù

Ïû•Í∏∞Ï†ÅÏúºÎ°ú convergence Ìå®ÌÑ¥ÏùÑ Ï∂îÏ†ÅÌïòÏó¨:
- ÏàòÎ†¥Î•† Î∂ÑÏÑù
- Threshold ÌäúÎãù Îç∞Ïù¥ÌÑ∞ Ï∂ïÏ†Å
- Ïã§Ìå® ÏºÄÏù¥Ïä§ Î∂ÑÏÑù
"""

import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path


class ConvergenceTracker:
    """
    Convergence pattern tracker

    ÏàòÎ†¥ Ìå®ÌÑ¥ÏùÑ Ï∂îÏ†ÅÌïòÍ≥† Î∂ÑÏÑùÌïòÏó¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌíàÏßà Î™®ÎãàÌÑ∞ÎßÅ
    """

    def __init__(self, log_file: Optional[str] = None):
        """
        Args:
            log_file: Î°úÍ∑∏ ÌååÏùº Í≤ΩÎ°ú (NoneÏù¥Î©¥ Î©îÎ™®Î¶¨ÏóêÎßå Ï†ÄÏû•)
        """
        self.convergence_history: List[Dict[str, Any]] = []
        self.log_file = log_file

        # Í∏∞Ï°¥ Î°úÍ∑∏ Î°úÎìú
        if log_file and Path(log_file).exists():
            self._load_from_file()

    def log_convergence(
        self,
        match_id: str,
        match_input: Dict[str, Any],
        convergence_info: Dict[str, Any],
        iterations: int
    ):
        """
        ÏàòÎ†¥ Ï†ïÎ≥¥ Í∏∞Î°ù

        Args:
            match_id: Í≤ΩÍ∏∞ ID
            match_input: Í≤ΩÍ∏∞ ÏûÖÎ†• Ï†ïÎ≥¥ (MatchInput.to_dict())
            convergence_info: ÏàòÎ†¥ Ï†ïÎ≥¥
            iterations: Ïã§Ï†ú Î∞òÎ≥µ ÌöüÏàò
        """
        # Ï†ÑÎ†• Ï∞®Ïù¥ Í≥ÑÏÇ∞
        strength_diff = self._calculate_strength_diff(match_input)

        # Ïä§ÌÉÄÏùº Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        style_similarity = self._calculate_style_similarity(match_input)

        # Î°úÍ∑∏ ÏóîÌä∏Î¶¨ ÏÉùÏÑ±
        log_entry = {
            'match_id': match_id,
            'timestamp': datetime.now().isoformat(),
            'home_team': match_input.get('home_team', {}).get('name', 'Unknown'),
            'away_team': match_input.get('away_team', {}).get('name', 'Unknown'),
            'strength_diff': strength_diff,
            'style_similarity': style_similarity,
            'converged': convergence_info.get('is_converged', False),
            'convergence_score': convergence_info.get('weighted_score', 0.0),
            'iterations': iterations,
            'threshold': convergence_info.get('threshold', 0.7),
            'early_stopped': convergence_info.get('early_stopped', False),
            'convergence_fallback': convergence_info.get('convergence_fallback', False),
            'uncertainty': convergence_info.get('uncertainty', 0.0),
        }

        self.convergence_history.append(log_entry)

        # ÌååÏùºÏóê Ï†ÄÏû• (append mode)
        if self.log_file:
            self._append_to_file(log_entry)

    def generate_report(self) -> Dict[str, Any]:
        """
        ÏàòÎ†¥ Ìå®ÌÑ¥ Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±

        Returns:
            Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÎîïÏÖîÎÑàÎ¶¨
        """
        if not self.convergence_history:
            return {
                'total_simulations': 0,
                'message': 'No data available'
            }

        # Í∏∞Î≥∏ ÌÜµÍ≥Ñ
        total = len(self.convergence_history)
        converged_count = sum(1 for entry in self.convergence_history if entry['converged'])
        early_stopped_count = sum(1 for entry in self.convergence_history if entry['early_stopped'])
        fallback_count = sum(1 for entry in self.convergence_history if entry['convergence_fallback'])

        # Î∞òÎ≥µ ÌöüÏàò ÌÜµÍ≥Ñ
        iterations_list = [entry['iterations'] for entry in self.convergence_history]
        avg_iterations = sum(iterations_list) / len(iterations_list)

        # ÏàòÎ†¥ Ï†êÏàò ÌÜµÍ≥Ñ
        scores = [entry['convergence_score'] for entry in self.convergence_history]
        avg_score = sum(scores) / len(scores)

        # Ï†ÑÎ†• Ï∞®Ïù¥Î≥Ñ ÏàòÎ†¥Î•†
        convergence_by_strength = self._analyze_by_strength_diff()

        # Ïã§Ìå®Ìïú Í≤ΩÍ∏∞ Î™©Î°ù
        failed_matches = [
            {
                'match_id': entry['match_id'],
                'teams': f"{entry['home_team']} vs {entry['away_team']}",
                'score': entry['convergence_score'],
                'iterations': entry['iterations']
            }
            for entry in self.convergence_history
            if not entry['converged']
        ]

        return {
            'total_simulations': total,
            'overall_convergence_rate': converged_count / total if total > 0 else 0,
            'early_stop_rate': early_stopped_count / total if total > 0 else 0,
            'fallback_rate': fallback_count / total if total > 0 else 0,
            'avg_iterations': avg_iterations,
            'avg_convergence_score': avg_score,
            'convergence_by_strength_diff': convergence_by_strength,
            'failed_matches': failed_matches,
            'recent_10': self.convergence_history[-10:] if len(self.convergence_history) >= 10 else self.convergence_history
        }

    def print_summary(self):
        """ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ Ï∂úÎ†•"""
        report = self.generate_report()

        if report['total_simulations'] == 0:
            print("No convergence data available.")
            return

        print("\n" + "=" * 70)
        print("üìä Convergence Tracking Summary")
        print("=" * 70)

        print(f"\nÏ¥ù ÏãúÎÆ¨Î†àÏù¥ÏÖò: {report['total_simulations']}")
        print(f"ÏàòÎ†¥Î•†: {report['overall_convergence_rate']:.1%}")
        print(f"Early Stop ÎπÑÏú®: {report['early_stop_rate']:.1%}")
        print(f"Fallback ÎπÑÏú®: {report['fallback_rate']:.1%}")
        print(f"ÌèâÍ∑† Î∞òÎ≥µ ÌöüÏàò: {report['avg_iterations']:.1f}")
        print(f"ÌèâÍ∑† ÏàòÎ†¥ Ï†êÏàò: {report['avg_convergence_score']:.2f}")

        print("\nÏ†ÑÎ†• Ï∞®Ïù¥Î≥Ñ ÏàòÎ†¥Î•†:")
        for range_label, rate in report['convergence_by_strength_diff'].items():
            print(f"  {range_label}: {rate:.1%}")

        if report['failed_matches']:
            print(f"\nÏã§Ìå®Ìïú Í≤ΩÍ∏∞ ({len(report['failed_matches'])}Í∞ú):")
            for match in report['failed_matches'][:5]:  # ÏµúÎåÄ 5Í∞úÎßå Ï∂úÎ†•
                print(f"  - {match['teams']} (Score: {match['score']:.2f}, Iterations: {match['iterations']})")

        print("\n" + "=" * 70)

    def _calculate_strength_diff(self, match_input: Dict[str, Any]) -> float:
        """Ï†ÑÎ†• Ï∞®Ïù¥ Í≥ÑÏÇ∞"""
        home_team = match_input.get('home_team', {})
        away_team = match_input.get('away_team', {})

        home_attack = home_team.get('attack_strength', 75)
        home_defense = home_team.get('defense_strength', 75)
        away_attack = away_team.get('attack_strength', 75)
        away_defense = away_team.get('defense_strength', 75)

        home_strength = (home_attack + home_defense) / 2
        away_strength = (away_attack + away_defense) / 2

        return abs(home_strength - away_strength)

    def _calculate_style_similarity(self, match_input: Dict[str, Any]) -> float:
        """Ïä§ÌÉÄÏùº Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        home_team = match_input.get('home_team', {})
        away_team = match_input.get('away_team', {})

        home_style = home_team.get('buildup_style', 'mixed').lower()
        away_style = away_team.get('buildup_style', 'mixed').lower()

        style_map = {"direct": 0, "mixed": 0.5, "possession": 1}
        value1 = style_map.get(home_style, 0.5)
        value2 = style_map.get(away_style, 0.5)

        return 1 - abs(value1 - value2)

    def _analyze_by_strength_diff(self) -> Dict[str, float]:
        """Ï†ÑÎ†• Ï∞®Ïù¥Î≥Ñ ÏàòÎ†¥Î•† Î∂ÑÏÑù"""
        ranges = {
            '0-10': (0, 10),
            '10-20': (10, 20),
            '20+': (20, 100)
        }

        results = {}
        for range_label, (min_diff, max_diff) in ranges.items():
            matches_in_range = [
                entry for entry in self.convergence_history
                if min_diff <= entry['strength_diff'] < max_diff
            ]

            if matches_in_range:
                converged_in_range = sum(1 for m in matches_in_range if m['converged'])
                results[range_label] = converged_in_range / len(matches_in_range)
            else:
                results[range_label] = 0.0

        return results

    def _load_from_file(self):
        """ÌååÏùºÏóêÏÑú Î°úÍ∑∏ Î°úÎìú"""
        try:
            with open(self.log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        entry = json.loads(line)
                        self.convergence_history.append(entry)
        except Exception as e:
            print(f"Warning: Failed to load convergence log: {e}")

    def _append_to_file(self, log_entry: Dict[str, Any]):
        """ÌååÏùºÏóê Î°úÍ∑∏ Ï∂îÍ∞Ä"""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"Warning: Failed to write convergence log: {e}")


# ==========================================================================
# Testing
# ==========================================================================

def test_convergence_tracker():
    """Convergence Tracker ÌÖåÏä§Ìä∏"""
    print("=== Convergence Tracker ÌÖåÏä§Ìä∏ ===\n")

    tracker = ConvergenceTracker()

    # Test 1: ÏàòÎ†¥ ÏÑ±Í≥µ ÏºÄÏù¥Ïä§
    match_input_1 = {
        'home_team': {
            'name': 'Man City',
            'attack_strength': 95,
            'defense_strength': 90,
            'buildup_style': 'possession'
        },
        'away_team': {
            'name': 'Sheffield',
            'attack_strength': 65,
            'defense_strength': 68,
            'buildup_style': 'direct'
        }
    }

    convergence_info_1 = {
        'is_converged': True,
        'weighted_score': 0.75,
        'early_stopped': True,
        'convergence_fallback': False,
        'uncertainty': 0.25
    }

    tracker.log_convergence(
        match_id='TEST_001',
        match_input=match_input_1,
        convergence_info=convergence_info_1,
        iterations=3
    )

    # Test 2: ÏàòÎ†¥ Ïã§Ìå® ÏºÄÏù¥Ïä§
    match_input_2 = {
        'home_team': {
            'name': 'Arsenal',
            'attack_strength': 88,
            'defense_strength': 85,
            'buildup_style': 'possession'
        },
        'away_team': {
            'name': 'Liverpool',
            'attack_strength': 87,
            'defense_strength': 84,
            'buildup_style': 'possession'
        }
    }

    convergence_info_2 = {
        'is_converged': False,
        'weighted_score': 0.55,
        'early_stopped': False,
        'convergence_fallback': True,
        'uncertainty': 0.45
    }

    tracker.log_convergence(
        match_id='TEST_002',
        match_input=match_input_2,
        convergence_info=convergence_info_2,
        iterations=5
    )

    # Test 3: Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
    tracker.print_summary()

    # Í≤ÄÏ¶ù
    report = tracker.generate_report()
    assert report['total_simulations'] == 2
    assert report['overall_convergence_rate'] == 0.5  # 1/2
    assert report['early_stop_rate'] == 0.5  # 1/2
    assert report['fallback_rate'] == 0.5  # 1/2

    print("\n‚úÖ Convergence Tracker ÌÖåÏä§Ìä∏ ÌÜµÍ≥º!")


if __name__ == "__main__":
    test_convergence_tracker()
