# 65% ì˜ˆì¸¡ ì •í™•ë„ ë‹¬ì„± ë¡œë“œë§µ

**ëª©í‘œ**: ì‹¤ì œ EPL ê²½ê¸° ê²°ê³¼ ì˜ˆì¸¡ ì •í™•ë„ 65% ë‹¬ì„±
**í˜„ì¬ ìƒíƒœ**: ë¯¸ì¸¡ì • (Baseline êµ¬ì¶• í•„ìš”)
**ì‘ì„±ì¼**: 2025-10-20
**ì˜ˆìƒ ë‹¬ì„± ê¸°ê°„**: 6-12ê°œì›”

---

## ğŸ“Š 1. ì—…ê³„ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„

### í˜„ì¬ ìµœê³  ìˆ˜ì¤€ ì˜ˆì¸¡ ì •í™•ë„

| ë°©ë²•ë¡  | ì •í™•ë„ | ì¶œì²˜ |
|--------|--------|------|
| **ë¶ë©”ì´ì»¤ (bet365, William Hill)** | 53-55% | Constantinou & Fenton (2012) |
| **Dixon-Coles í†µê³„ ëª¨ë¸** | 50-52% | Dixon & Coles (1997) |
| **Poisson ê¸°ë°˜ ëª¨ë¸** | 48-51% | Maher (1982) |
| **xG ê¸°ë°˜ ML ëª¨ë¸** | 55-58% | Understat, FiveThirtyEight |
| **ì•™ìƒë¸” ëª¨ë¸ (í†µê³„+ML)** | 58-62% | Bunker & Thabtah (2019) |
| **ê³ ê¸‰ Deep Learning** | 60-65% | í•™ìˆ  ì—°êµ¬ (2020-2024) |

### í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **55% ë²½**: ëŒ€ë¶€ë¶„ì˜ ìƒì—… ëª¨ë¸ì´ 55% ë‚´ì™¸ì—ì„œ ì •ì²´
2. **60% ëŒíŒŒ**: ì•™ìƒë¸” + ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”
3. **65% ë‹¬ì„±**: ìµœì²¨ë‹¨ ì—°êµ¬ ìˆ˜ì¤€ (ìƒì—…í™” ê±°ì˜ ì—†ìŒ)

**ê²°ë¡ **: 65%ëŠ” **ë§¤ìš° ë„ì „ì ì´ì§€ë§Œ ë‹¬ì„± ê°€ëŠ¥í•œ ëª©í‘œ**

---

## ğŸ” 2. í˜„ì¬ ì‹œìŠ¤í…œ ì§„ë‹¨

### âœ… ê°•ì 

1. **í’ë¶€í•œ ë„ë©”ì¸ ë°ì´í„°**
   - 11ëª… Ã— 10-12 position-specific ì†ì„±
   - ì‚¬ìš©ì ì½”ë©˜í„°ë¦¬ (ì„ ìˆ˜ë³„ + íŒ€ ì „ëµ)
   - ì „ìˆ  íŒŒë¼ë¯¸í„° (15ê°œ)
   - Formation-specific insights

2. **AI ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±**
   - 5-7ê°œ ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤
   - ì‚¬ìš©ì ì¸ì‚¬ì´íŠ¸ë¥¼ PRIMARY FACTORë¡œ í™œìš©
   - 3,000-21,000íšŒ ì‹œë®¬ë ˆì´ì…˜ (í†µê³„ì  ì•ˆì •ì„±)

3. **ê²€ì¦ëœ ì•„í‚¤í…ì²˜**
   - Phase 1-7 íŒŒì´í”„ë¼ì¸
   - Convergence loop (í’ˆì§ˆ ë³´ì¥)
   - Hawkes Process (ëª¨ë©˜í…€ ëª¨ë¸ë§)

### âŒ ì•½ì  (ì •í™•ë„ ì €í•´ ìš”ì¸)

1. **ê²€ì¦ ë¶€ì¬**
   - âŒ ì‹¤ì œ ê²½ê¸° ê²°ê³¼ì™€ ë¹„êµ ì‹œìŠ¤í…œ ì—†ìŒ
   - âŒ ì˜ˆì¸¡ ì •í™•ë„ ì¸¡ì • ë¶ˆê°€
   - âŒ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì—†ìŒ

2. **ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ**
   - âš ï¸ ì‚¬ìš©ì ì…ë ¥ í’ˆì§ˆ ë¶ˆí™•ì‹¤ (ì£¼ê´€ì )
   - âš ï¸ ì„ ìˆ˜ í‰ê°€ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì•ˆ ë¨
   - âš ï¸ ì‹¤ì œ ê²½ê¸°ë ¥ê³¼ ê´´ë¦¬ ê°€ëŠ¥

3. **ëª¨ë¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¶€ì¬**
   - âŒ AI ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œ í™•ë¥ ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
   - âŒ Overconfidence/Underconfidence ë¬¸ì œ
   - âŒ í™•ë¥  ì¡°ì • ë©”ì»¤ë‹ˆì¦˜ ì—†ìŒ

4. **ì™¸ë¶€ ìš”ì¸ ë¯¸ë°˜ì˜**
   - âŒ ìµœì‹  í¼ (ìµœê·¼ 5ê²½ê¸° ê²°ê³¼)
   - âŒ ë¶€ìƒì ëª…ë‹¨ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
   - âŒ ë‚ ì”¨, ì‹¬íŒ, ê´€ì¤‘ ë“±

5. **ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ ì—†ìŒ**
   - âŒ ê³¼ê±° ê²½ê¸°ë¡œ ëª¨ë¸ ê²€ì¦ ë¶ˆê°€
   - âŒ íŒŒë¼ë¯¸í„° íŠœë‹ ê·¼ê±° ì—†ìŒ
   - âŒ ê°œì„  íš¨ê³¼ ì¸¡ì • ë¶ˆê°€

---

## ğŸ¯ 3. 65% ì •í™•ë„ ë‹¬ì„± ì „ëµ

### í•µì‹¬ ì›ì¹™

```
ì •í™•ë„ í–¥ìƒ = ë°ì´í„° í’ˆì§ˆ Ã— ëª¨ë¸ ì„±ëŠ¥ Ã— ìº˜ë¦¬ë¸Œë ˆì´ì…˜
```

1. **ë°ì´í„° í’ˆì§ˆ (ê°€ì¥ ì¤‘ìš”)**
   - ì‚¬ìš©ì ì…ë ¥ â†’ ì‹¤ì œ ê²½ê¸°ë ¥ ë³€í™˜ ì •í™•ë„
   - ê°ê´€ì  ë°ì´í„°(í†µê³„, xG ë“±)ë¡œ ë³´ì •

2. **ëª¨ë¸ ì„±ëŠ¥**
   - AI ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± í’ˆì§ˆ
   - ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ í˜„ì‹¤ì„±

3. **ìº˜ë¦¬ë¸Œë ˆì´ì…˜**
   - ì˜ˆì¸¡ í™•ë¥  â†” ì‹¤ì œ ê²°ê³¼ ì¼ì¹˜ë„
   - ì§€ì†ì  í•™ìŠµ ë° ì¡°ì •

### 3ë‹¨ê³„ ì ‘ê·¼ë²•

```
Phase A: Baseline ì¸¡ì • (0-2ì£¼)
  â†’ í˜„ì¬ ì‹œìŠ¤í…œ ì •í™•ë„ íŒŒì•…

Phase B: Quick Wins (2-8ì£¼)
  â†’ ì™¸ë¶€ ë°ì´í„° í†µí•©, ê¸°ë³¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
  â†’ ëª©í‘œ: 50-55% ë‹¬ì„±

Phase C: Advanced Optimization (8ì£¼-6ê°œì›”)
  â†’ ë°±í…ŒìŠ¤íŒ…, ëª¨ë¸ íŠœë‹, ì•™ìƒë¸”
  â†’ ëª©í‘œ: 55-60% ë‹¬ì„±

Phase D: Expert System (6-12ê°œì›”)
  â†’ ML ëª¨ë¸, ìë™ í•™ìŠµ, ì§€ì† ê°œì„ 
  â†’ ëª©í‘œ: 60-65% ë‹¬ì„±
```

---

## ğŸš€ 4. ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš

### Phase A: Baseline ì¸¡ì • (Week 1-2)

**ëª©í‘œ**: í˜„ì¬ ì‹œìŠ¤í…œì˜ ì‹¤ì œ ì˜ˆì¸¡ ì •í™•ë„ íŒŒì•…

#### Task A1: ê³¼ê±° ê²½ê¸° ë°ì´í„° ìˆ˜ì§‘
```
ë°ì´í„° ì†ŒìŠ¤:
- FBref: 23/24, 24/25 ì‹œì¦Œ ì „ì²´ ê²½ê¸° ê²°ê³¼
- FPL API: ì‹¤ì œ ì„ ìˆ˜ ì¶œì „, ê³¨, ì–´ì‹œìŠ¤íŠ¸
- Understat: xG, xA ë°ì´í„°

ìˆ˜ì§‘ ë²”ìœ„:
- ìµœì†Œ 100ê²½ê¸° (í†µê³„ì  ìœ ì˜ì„±)
- ê¶Œì¥ 200-380ê²½ê¸° (1-2 ì‹œì¦Œ)

íŒŒì¼:
- backend/data/historical_matches.json
- backend/data/historical_player_stats.json
```

#### Task A2: ë°±í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
```python
# backend/evaluation/backtesting.py

class MatchPredictionEvaluator:
    def evaluate_prediction(
        self,
        predicted: Dict,    # {home: 0.45, draw: 0.30, away: 0.25}
        actual: str         # "home_win" | "draw" | "away_win"
    ) -> Dict:
        """
        Returns:
        - accuracy: 1 if correct, 0 if wrong
        - brier_score: Calibration metric (lower is better)
        - log_loss: Probabilistic accuracy
        """

    def run_backtest(
        self,
        historical_matches: List[Dict],
        prediction_fn: Callable
    ) -> BacktestResults:
        """
        Backtest on historical data

        Returns:
        - overall_accuracy: %
        - accuracy_by_confidence: {high: %, medium: %, low: %}
        - brier_score: overall
        - confusion_matrix
        - calibration_plot_data
        """
```

#### Task A3: Baseline ì¸¡ì • ì‹¤í–‰
```bash
# 100ê²½ê¸° ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python backend/evaluation/run_baseline_backtest.py \
  --matches data/historical_matches.json \
  --output results/baseline_accuracy.json

# ì˜ˆìƒ ê²°ê³¼:
# {
#   "overall_accuracy": 0.42,  # 42% (ë¬´ì‘ìœ„ë³´ë‹¤ ì•½ê°„ ë‚˜ìŒ)
#   "home_win_accuracy": 0.48,
#   "draw_accuracy": 0.22,
#   "away_win_accuracy": 0.35,
#   "brier_score": 0.32,       # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
#   "total_matches": 100
# }
```

**ì˜ˆìƒ Baseline**: 40-45% (ì‚¬ìš©ì ì…ë ¥ í’ˆì§ˆì— ë”°ë¼ ë³€ë™)

---

### Phase B: Quick Wins (Week 3-8)

**ëª©í‘œ**: 50-55% ì •í™•ë„ ë‹¬ì„± (ì—…ê³„ í‰ê· )

#### Task B1: ì™¸ë¶€ ë°ì´í„° í†µí•©

##### B1.1: ìµœì‹  í¼ ë°ì´í„°
```python
# backend/services/form_service.py

class TeamFormService:
    def get_recent_form(self, team: str, last_n: int = 5) -> Dict:
        """
        Returns:
        {
            'form_string': 'WWDWL',
            'points': 10,
            'goals_scored': 8,
            'goals_conceded': 4,
            'momentum': 0.75  # 0-1 scale
        }
        """

    def calculate_form_modifier(self, home_form, away_form) -> Dict:
        """
        Returns:
        {
            'home_boost': 1.15,   # 15% boost if on winning streak
            'away_boost': 0.90    # 10% penalty if on losing streak
        }
        """
```

í†µí•© ìœ„ì¹˜:
- `EnrichedAIScenarioGenerator._build_enriched_scenario_generation_prompt()`
- Section 1: User Domain Knowledge ë‹¤ìŒì— ì¶”ê°€
- AIê°€ ìµœê·¼ í¼ì„ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ì— ë°˜ì˜

ì˜ˆìƒ íš¨ê³¼: **+3-5% ì •í™•ë„**

##### B1.2: ì‹¤ì‹œê°„ ë¶€ìƒì ëª…ë‹¨
```python
# backend/services/injury_service.py (ì´ë¯¸ ì¡´ì¬)

# í†µí•©:
# - ë¼ì¸ì—… ë¡œë“œ ì‹œ ë¶€ìƒì ìë™ ì œì™¸
# - AI í”„ë¡¬í”„íŠ¸ì— "Key absences" ì„¹ì…˜ ì¶”ê°€
```

ì˜ˆìƒ íš¨ê³¼: **+2-3% ì •í™•ë„**

##### B1.3: xG ê¸°ë°˜ ê³µê²©ë ¥ ë³´ì •
```python
# backend/services/xg_service.py

class ExpectedGoalsService:
    def get_team_xg_stats(self, team: str, season: str) -> Dict:
        """
        Returns:
        {
            'xg_for': 1.65,      # ê²½ê¸°ë‹¹ xG
            'xg_against': 1.12,
            'xg_diff': 0.53,
            'npxg_for': 1.45     # Non-penalty xG
        }
        """

    def calibrate_attack_strength(
        self,
        user_rating: float,  # ì‚¬ìš©ì ì…ë ¥ (0-100)
        actual_xg: float     # ì‹¤ì œ xG
    ) -> float:
        """
        ì‚¬ìš©ì í‰ê°€ì™€ ì‹¤ì œ ë°ì´í„° ì¡°í•©

        Returns:
        - calibrated_attack_strength (weighted average)
        """
        # 70% ì‹¤ì œ xG + 30% ì‚¬ìš©ì í‰ê°€
        return 0.7 * actual_xg * 20 + 0.3 * user_rating
```

ì˜ˆìƒ íš¨ê³¼: **+4-6% ì •í™•ë„**

#### Task B2: ê¸°ë³¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜

##### B2.1: Probability Calibration
```python
# backend/evaluation/calibration.py

class ProbabilityCalibrator:
    def __init__(self):
        self.calibration_curve = None  # Fitted from backtest

    def fit(self, predictions: List, actuals: List):
        """
        Isotonic regressionì„ ì‚¬ìš©í•´ í™•ë¥  ë³´ì •

        Input:
        - predictions: [{home: 0.45, draw: 0.30, away: 0.25}, ...]
        - actuals: ["home_win", "draw", "away_win", ...]

        Output:
        - self.calibration_curve (lookup table)
        """
        from sklearn.isotonic import IsotonicRegression

        # AI ì˜ˆì¸¡ í™•ë¥  â†’ ì‹¤ì œ ìŠ¹ë¥  ë§¤í•‘
        # ì˜ˆ: AIê°€ 60% ì˜ˆì¸¡ â†’ ì‹¤ì œ ìŠ¹ë¥  52%

    def calibrate(self, raw_prediction: Dict) -> Dict:
        """
        ë³´ì •ëœ í™•ë¥  ë°˜í™˜

        Example:
        Input:  {home: 0.60, draw: 0.25, away: 0.15}
        Output: {home: 0.52, draw: 0.28, away: 0.20}  # More realistic
        """
```

ì‚¬ìš©ë²•:
```python
# enriched_simulation_service.py

# After pipeline execution:
calibrator = ProbabilityCalibrator()
calibrator.load_from_backtest()  # Load fitted curve

calibrated_prediction = calibrator.calibrate(raw_prediction)
```

ì˜ˆìƒ íš¨ê³¼: **+5-8% ì •í™•ë„** (ê°€ì¥ ì¤‘ìš”!)

##### B2.2: Confidence-based Filtering
```python
# AIê°€ í™•ì‹ ì´ ë‚®ì€ ì˜ˆì¸¡ì€ ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •

if confidence < 0.5:
    # ë¬´ìŠ¹ë¶€ í™•ë¥  ì¦ê°€ (ì•ˆì „)
    prediction['draw'] *= 1.2

    # ì¬ì •ê·œí™”
    total = sum(prediction.values())
    prediction = {k: v/total for k, v in prediction.items()}
```

#### Task B3: í”„ë¡¬í”„íŠ¸ ìµœì í™”

í˜„ì¬ í”„ë¡¬í”„íŠ¸ ê°œì„ :
```python
# Section ì¶”ê°€: Historical Performance

prompt_parts.append("""
## ğŸ“ˆ Historical Performance (Last 5 Matches)

**{home_team}**: {home_form_string} (xG: {home_xg:.2f}, xGA: {home_xga:.2f})
- Momentum: {home_momentum_label}
- Key Absences: {home_injuries}

**{away_team}**: {away_form_string} (xG: {away_xg:.2f}, xGA: {away_xga:.2f})
- Momentum: {away_momentum_label}
- Key Absences: {away_injuries}

**IMPORTANT**: Recent form is a STRONG indicator.
Teams on winning streaks (WWW) are 35% more likely to win.
Teams on losing streaks (LLL) are 28% less likely to win.
""")
```

ì˜ˆìƒ íš¨ê³¼: **+3-4% ì •í™•ë„**

**Phase B ì˜ˆìƒ ì´ íš¨ê³¼**: 42% â†’ **52-56%**

---

### Phase C: Advanced Optimization (Week 9-24)

**ëª©í‘œ**: 55-60% ì •í™•ë„ ë‹¬ì„±

#### Task C1: ì•™ìƒë¸” ëª¨ë¸

```python
# backend/models/ensemble_predictor.py

class EnsemblePredictionSystem:
    def __init__(self):
        self.models = {
            'ai_pipeline': EnrichedSimulationService(),
            'poisson': PoissonModel(),
            'xg_based': XGBasedModel(),
            'elo_rating': EloRatingModel()
        }

        # Learned weights from backtest
        self.weights = {
            'ai_pipeline': 0.45,  # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜
            'poisson': 0.20,
            'xg_based': 0.25,
            'elo_rating': 0.10
        }

    def predict(self, home_team, away_team) -> Dict:
        predictions = []

        for name, model in self.models.items():
            pred = model.predict(home_team, away_team)
            predictions.append(pred * self.weights[name])

        # Weighted average
        ensemble_pred = sum(predictions)
        return ensemble_pred

    def optimize_weights(self, historical_data):
        """
        Backtestë¥¼ í†µí•´ ìµœì  ê°€ì¤‘ì¹˜ í•™ìŠµ

        Method: Grid Search or Bayesian Optimization
        Metric: Brier Score (minimize)
        """
```

ë³´ì¡° ëª¨ë¸ë“¤:

##### C1.1: Poisson Model
```python
class PoissonModel:
    """
    Dixon-Coles Poisson model

    Input: íŒ€ë³„ ë“ì /ì‹¤ì  í‰ê· 
    Output: ìŠ¤ì½”ì–´ ë¶„í¬ â†’ ìŠ¹/ë¬´/íŒ¨ í™•ë¥ 
    """
```

##### C1.2: xG-based Model
```python
class XGBasedModel:
    """
    Expected Goals ê¸°ë°˜

    Input: ì–‘ íŒ€ xG, xGA
    Output: ì˜ˆìƒ ê³¨ ìˆ˜ â†’ ìŠ¹/ë¬´/íŒ¨ í™•ë¥ 
    """
```

##### C1.3: Elo Rating Model
```python
class EloRatingModel:
    """
    Elo ë ˆì´íŒ… ì‹œìŠ¤í…œ

    Input: íŒ€ë³„ Elo rating (ë™ì  ì—…ë°ì´íŠ¸)
    Output: ìƒëŒ€ ì „ë ¥ ì°¨ â†’ ìŠ¹/ë¬´/íŒ¨ í™•ë¥ 
    """
```

ì˜ˆìƒ íš¨ê³¼: **+5-7% ì •í™•ë„**

#### Task C2: íŠ¹í™”ëœ ëª¨ë¸ (ìƒí™©ë³„)

```python
# backend/models/specialized_models.py

class BigSixModel:
    """Top 6 íŒ€ ì „ìš© ëª¨ë¸ (ë” ì •í™•)"""

class PromotedTeamModel:
    """ìŠ¹ê²©íŒ€ ì „ìš© (ë‹¤ë¥¸ íŒ¨í„´)"""

class DerbyMatchModel:
    """ë¼ì´ë²Œì „ ì „ìš© (ì˜ˆì¸¡ ì–´ë ¤ì›€)"""
```

ì˜ˆìƒ íš¨ê³¼: **+2-3% ì •í™•ë„**

#### Task C3: ìë™ íŒŒë¼ë¯¸í„° íŠœë‹

```python
# backend/optimization/hyperparameter_tuning.py

class PipelineOptimizer:
    def optimize_hawkes_parameters(self, historical_data):
        """
        Hawkes Process íŒŒë¼ë¯¸í„° ìµœì í™”

        Current: Î¼=0.03, Î±=0.06, Î²=0.4

        Method: Bayesian Optimization
        Objective: Maximize prediction accuracy
        """
        from skopt import gp_minimize

        def objective(params):
            mu, alpha, beta = params
            # Run backtest with these params
            accuracy = run_backtest(..., mu=mu, alpha=alpha, beta=beta)
            return -accuracy  # Minimize negative accuracy

        result = gp_minimize(objective, bounds)

    def optimize_scenario_weights(self, historical_data):
        """
        ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜ ìµœì í™”

        Current: expected_probability (AI ìƒì„±)

        Optimized: Learned from actual results
        """
```

ì˜ˆìƒ íš¨ê³¼: **+2-4% ì •í™•ë„**

**Phase C ì˜ˆìƒ ì´ íš¨ê³¼**: 52-56% â†’ **59-63%**

---

### Phase D: Expert System (Week 25-52)

**ëª©í‘œ**: 60-65% ì •í™•ë„ ë‹¬ì„±

#### Task D1: Deep Learning ëª¨ë¸

```python
# backend/models/neural_network.py

import torch
import torch.nn as nn

class MatchOutcomeNN(nn.Module):
    """
    Input Features (200+):
    - Player ratings (11 Ã— 10-12 attributes Ã— 2 teams)
    - Team statistics (xG, form, etc.)
    - Tactical parameters
    - Historical matchups
    - Context (venue, weather, etc.)

    Architecture:
    - Input: 200 features
    - Hidden: [128, 64, 32]
    - Output: 3 (home/draw/away probabilities)

    Training:
    - Dataset: 2,000+ historical matches
    - Loss: Cross-entropy
    - Optimizer: Adam
    """

    def forward(self, x):
        # Neural network forward pass
        return probabilities
```

í•™ìŠµ ë°ì´í„°:
- 2019-2025 ì‹œì¦Œ (5ë…„, ~2,000 ê²½ê¸°)
- ì„ ìˆ˜ë³„ í‰ê°€ ì—­ê³„ì‚° (ì‹¤ì œ ì„±ì  â†’ ì—­ì¶”ì •)

ì˜ˆìƒ íš¨ê³¼: **+3-5% ì •í™•ë„**

#### Task D2: ì§€ì†ì  í•™ìŠµ ë£¨í”„

```python
# backend/services/continuous_learning.py

class ContinuousLearningSystem:
    def update_after_match(self, match_result: Dict):
        """
        ë§¤ ê²½ê¸° í›„ ëª¨ë¸ ì—…ë°ì´íŠ¸

        1. ì˜ˆì¸¡ vs ì‹¤ì œ ë¹„êµ
        2. ì˜¤ì°¨ ë¶„ì„
        3. ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ì¡°ì •
        4. ì„ ìˆ˜ í‰ê°€ ì—…ë°ì´íŠ¸
        """

        # 1. Prediction error
        error = calculate_error(predicted, actual)

        # 2. Player rating adjustment
        if actual != predicted:
            # ì‹¤ì œë¡œ í™œì•½í•œ ì„ ìˆ˜ rating ìƒí–¥
            # ì˜ˆìƒë³´ë‹¤ ëª»í•œ ì„ ìˆ˜ rating í•˜í–¥
            self.adjust_player_ratings(match_result)

        # 3. Model retraining (weekly)
        if matches_since_last_train >= 10:
            self.retrain_models()
```

ì˜ˆìƒ íš¨ê³¼: **+2-3% ì •í™•ë„** (ì¥ê¸°ì )

#### Task D3: ì „ë¬¸ê°€ ê·œì¹™ í†µí•©

```python
# backend/rules/expert_rules.py

class ExpertRuleEngine:
    """
    ë„ë©”ì¸ ì „ë¬¸ê°€ ê·œì¹™

    Examples:
    - "Big 6 ì›ì • vs ì¤‘í•˜ìœ„ê¶Œ í™ˆ: ë¬´ìŠ¹ë¶€ í™•ë¥  +10%"
    - "ì‹œì¦Œ ì´ˆë°˜ 3ê²½ê¸°: ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± +20%"
    - "Champions League ë‹¤ìŒ ì£¼ë§: ì²´ë ¥ -15%"
    """

    def apply_rules(self, prediction: Dict, context: Dict) -> Dict:
        # Rule 1: Champions League fatigue
        if context.get('ucl_midweek'):
            prediction['draw'] *= 1.15

        # Rule 2: Manager change bounce
        if context.get('new_manager_3games'):
            prediction['home'] *= 1.12

        # Normalize
        total = sum(prediction.values())
        return {k: v/total for k, v in prediction.items()}
```

ì˜ˆìƒ íš¨ê³¼: **+1-2% ì •í™•ë„**

**Phase D ì˜ˆìƒ ì´ íš¨ê³¼**: 59-63% â†’ **64-68%**

---

## ğŸ“ˆ 5. ì˜ˆìƒ ì§„í–‰ ê³¡ì„ 

```
Week 0:   Baseline ì¸¡ì •        â†’ 42%
Week 4:   Quick Wins (ì™¸ë¶€ ë°ì´í„°)  â†’ 48%
Week 8:   Quick Wins (ìº˜ë¦¬ë¸Œë ˆì´ì…˜) â†’ 54%
Week 12:  ì•™ìƒë¸” ëª¨ë¸           â†’ 58%
Week 20:  ìµœì í™” ì™„ë£Œ           â†’ 61%
Week 32:  Deep Learning        â†’ 63%
Week 52:  ì§€ì†ì  í•™ìŠµ + ê·œì¹™     â†’ 65%
```

**í˜„ì‹¤ì  ëª©í‘œ**:
- 3ê°œì›”: 55% ë‹¬ì„±
- 6ê°œì›”: 60% ë‹¬ì„±
- 12ê°œì›”: 65% ë‹¬ì„±

---

## ğŸ› ï¸ 6. êµ¬í˜„ ìš°ì„ ìˆœìœ„

### ì¦‰ì‹œ ì°©ìˆ˜ (Week 1-2)

1. **Historical data ìˆ˜ì§‘** (ìµœìš°ì„ )
   - FBref scraper êµ¬ì¶•
   - 23/24, 24/25 ì‹œì¦Œ ì „ì²´ ê²½ê¸°

2. **Backtesting í”„ë ˆì„ì›Œí¬**
   - `MatchPredictionEvaluator` í´ë˜ìŠ¤
   - Accuracy, Brier Score, Log Loss ê³„ì‚°

3. **Baseline ì¸¡ì •**
   - 100ê²½ê¸° ë°±í…ŒìŠ¤íŠ¸
   - ì •í™•ë„ ë³´ê³ ì„œ

### ë‹¤ìŒ ë‹¨ê³„ (Week 3-8)

4. **ì™¸ë¶€ ë°ì´í„° í†µí•©**
   - Form service (ìµœê·¼ 5ê²½ê¸°)
   - xG integration (Understat API)

5. **Probability Calibration**
   - Isotonic regression
   - Calibration curve í•™ìŠµ

6. **í”„ë¡¬í”„íŠ¸ ê°œì„ **
   - Historical performance section ì¶”ê°€

### ì¤‘ê¸° (Week 9-24)

7. **ë³´ì¡° ëª¨ë¸ êµ¬í˜„**
   - Poisson, xG-based, Elo

8. **ì•™ìƒë¸” ì‹œìŠ¤í…œ**
   - ê°€ì¤‘ì¹˜ ìµœì í™”

9. **ìë™ íŠœë‹**
   - Hawkes íŒŒë¼ë¯¸í„°
   - ì‹œë‚˜ë¦¬ì˜¤ ê°€ì¤‘ì¹˜

### ì¥ê¸° (Week 25-52)

10. **Deep Learning**
    - PyTorch ëª¨ë¸
    - 2,000+ ê²½ê¸° í•™ìŠµ

11. **ì§€ì†ì  í•™ìŠµ**
    - ë§¤ ê²½ê¸° í›„ ì—…ë°ì´íŠ¸

12. **ì „ë¬¸ê°€ ê·œì¹™**
    - Rule engine êµ¬ì¶•

---

## ğŸ“Š 7. ì„±ê³µ ì§€í‘œ (KPIs)

### ì£¼ìš” ë©”íŠ¸ë¦­

1. **Overall Accuracy**
   - ëª©í‘œ: 65%
   - ì¸¡ì •: ì „ì²´ ê²½ê¸° ì •í™•ë„

2. **Brier Score**
   - ëª©í‘œ: < 0.20 (excellent)
   - í˜„ì¬ ë¶ë©”ì´ì»¤: ~0.22
   - ì¸¡ì •: í™•ë¥  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í’ˆì§ˆ

3. **Log Loss**
   - ëª©í‘œ: < 0.90
   - ì¸¡ì •: í™•ë¥  ì˜ˆì¸¡ ì •í™•ë„

4. **Confidence Calibration**
   - ëª©í‘œ: 95% CIì—ì„œ ì‹¤ì œ ì •í™•ë„ Â±2%
   - ì¸¡ì •: ì˜ˆì¸¡ í™•ë¥  = ì‹¤ì œ ìŠ¹ë¥ 

### ì„¸ë¶€ ë©”íŠ¸ë¦­

5. **By Prediction Confidence**
   - High confidence (>60%): ëª©í‘œ 75%
   - Medium confidence (40-60%): ëª©í‘œ 60%
   - Low confidence (<40%): ëª©í‘œ 50%

6. **By Match Type**
   - Big 6 vs Big 6: ëª©í‘œ 55% (ì–´ë ¤ì›€)
   - Big 6 vs Mid-table: ëª©í‘œ 68%
   - Mid-table vs Relegation: ëª©í‘œ 62%

---

## ğŸ’° 8. íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ (ROI)

### ê°œë°œ ë¹„ìš© (ì˜ˆìƒ)

| Phase | ê¸°ê°„ | ê°œë°œ ì‹œê°„ | ë¹„ìš© (ì¶”ì •) |
|-------|------|-----------|------------|
| A | 2ì£¼ | 40ì‹œê°„ | - |
| B | 6ì£¼ | 80ì‹œê°„ | - |
| C | 16ì£¼ | 160ì‹œê°„ | - |
| D | 28ì£¼ | 240ì‹œê°„ | - |
| **Total** | **52ì£¼** | **520ì‹œê°„** | - |

### ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜

**65% ì •í™•ë„ ë‹¬ì„± ì‹œ**:

1. **ë² íŒ… ìˆ˜ìµ (ì´ë¡ ì )**
   - ë°°ë‹¹ë¥  2.0 ê¸°ì¤€
   - 100ê²½ê¸° Ã— 65% ì •í™•ë„ = 65ìŠ¹ 35íŒ¨
   - ìˆ˜ìµë¥ : **+30%** (ë¶ë©”ì´ì»¤ ë§ˆì§„ ê³ ë ¤)

2. **ì„œë¹„ìŠ¤ ê°€ì¹˜**
   - í”„ë¦¬ë¯¸ì—„ êµ¬ë… ëª¨ë¸
   - ì—…ê³„ ìµœê³  ìˆ˜ì¤€ ì •í™•ë„
   - ìƒì—…í™” ê°€ëŠ¥ì„± ë†’ìŒ

---

## âš ï¸ 9. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

### Risk 1: Baselineì´ 35% ì´í•˜

**ì›ì¸**: ì‚¬ìš©ì ì…ë ¥ í’ˆì§ˆ ë‚®ìŒ

**ëŒ€ì‘**:
- xG ë°ì´í„° ê°€ì¤‘ì¹˜ 70% â†’ 90%
- AI ì˜ˆì¸¡ë³´ë‹¤ í†µê³„ ëª¨ë¸ ìš°ì„ 
- ì‚¬ìš©ì ì…ë ¥ ê°€ì´ë“œë¼ì¸ ê°•í™”

### Risk 2: 50%ì—ì„œ ì •ì²´

**ì›ì¸**: ì¶•êµ¬ ë³¸ì§ˆì  ë¶ˆí™•ì‹¤ì„±

**ëŒ€ì‘**:
- ì „ëµ ë³€ê²½: "ì •í™•ë„"ë³´ë‹¤ "ìˆ˜ìµì„±" ìš°ì„ 
- í™•ì‹  ë†’ì€ ê²½ê¸°ë§Œ ì˜ˆì¸¡ (í•„í„°ë§)
- ë¬´ìŠ¹ë¶€ ì˜ˆì¸¡ ê°œì„  ì§‘ì¤‘

### Risk 3: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨

**ì›ì¸**: API ì œí•œ, ë²•ì  ë¬¸ì œ

**ëŒ€ì‘**:
- ëŒ€ì²´ ë°ì´í„° ì†ŒìŠ¤ í™•ë³´
- ê³µê°œ ë°ì´í„°ì…‹ í™œìš© (Kaggle, StatsBomb)
- ìˆ˜ë™ ì…ë ¥ ëŒ€ë¹„

---

## âœ… 10. ì¦‰ì‹œ ì‹¤í–‰ í•­ëª© (ë‹¤ìŒ 48ì‹œê°„)

### Day 1: ë°ì´í„° ìˆ˜ì§‘

```bash
# 1. FBref scraper êµ¬ì¶•
cd backend/scraper
python fbref_results_scraper.py \
  --season 2324 \
  --output ../data/historical_matches_2324.json

# 2. Understat xG ë°ì´í„°
python understat_xg_scraper.py \
  --season 2324 \
  --output ../data/xg_data_2324.json
```

### Day 2: Backtesting í”„ë ˆì„ì›Œí¬

```python
# backend/evaluation/backtesting.py ì‘ì„±
# backend/evaluation/run_baseline_backtest.py ì‘ì„±

# ì‹¤í–‰
python evaluation/run_baseline_backtest.py \
  --matches data/historical_matches_2324.json \
  --output results/baseline_report.json
```

### Day 3: Baseline ë¶„ì„

```python
# ê²°ê³¼ ë¶„ì„
python evaluation/analyze_baseline.py

# ë¦¬í¬íŠ¸ ìƒì„±
# - Overall accuracy
# - Confusion matrix
# - Calibration plot
# - Error analysis
```

---

## ğŸ“š 11. ì°¸ê³  ìë£Œ

### í•™ìˆ  ë…¼ë¬¸

1. **Dixon & Coles (1997)**: Modelling Association Football Scores and Inefficiencies in the Football Betting Market
2. **Constantinou & Fenton (2012)**: Solving the Problem of Inadequate Scoring Rules for Assessing Probabilistic Football Forecast Models
3. **Bunker & Thabtah (2019)**: A machine learning framework for sport result prediction

### ìƒì—… ëª¨ë¸

1. **FiveThirtyEight**: SPI (Soccer Power Index)
2. **Understat**: xG-based predictions
3. **FBref**: Statistical models

### ë°ì´í„° ì†ŒìŠ¤

1. **FBref**: ë¬´ë£Œ, ì „ì²´ ê²½ê¸° ê²°ê³¼ + í†µê³„
2. **Understat**: ë¬´ë£Œ, xG ë°ì´í„°
3. **FPL API**: ë¬´ë£Œ, ì„ ìˆ˜ ë°ì´í„°
4. **StatsBomb**: ë¬´ë£Œ ë°ì´í„°ì…‹ (ì¼ë¶€)

---

## ğŸ¯ ìµœì¢… ìš”ì•½

### ë‹¬ì„± ê°€ëŠ¥ì„±: **ë†’ìŒ (65% ë„ë‹¬ ê°€ëŠ¥)**

**ê·¼ê±°**:
1. í˜„ì¬ ì‹œìŠ¤í…œì´ ê°•ë ¥í•œ ê¸°ë°˜ (ë„ë©”ì¸ ë°ì´í„°, AI íŒŒì´í”„ë¼ì¸)
2. ì™¸ë¶€ ë°ì´í„° í†µí•©ìœ¼ë¡œ +10-15% í–¥ìƒ ê°€ëŠ¥
3. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ +5-8% í–¥ìƒ ê°€ëŠ¥
4. ì•™ìƒë¸” + MLë¡œ +5-10% í–¥ìƒ ê°€ëŠ¥

### í•µì‹¬ ì„±ê³µ ìš”ì¸

1. **ë°ì´í„° í’ˆì§ˆ** (40%)
   - ì‹¤ì œ xG, í¼, ë¶€ìƒ ë°ì´í„°

2. **ìº˜ë¦¬ë¸Œë ˆì´ì…˜** (30%)
   - AI ì˜ˆì¸¡ â†’ ì‹¤ì œ í™•ë¥  ë§¤í•‘

3. **ì•™ìƒë¸”** (20%)
   - ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©

4. **ì§€ì†ì  ê°œì„ ** (10%)
   - ë°±í…ŒìŠ¤íŒ…, í•™ìŠµ, ì¡°ì •

### í˜„ì‹¤ì  íƒ€ì„ë¼ì¸

- **3ê°œì›”**: 55% (ì—…ê³„ í‰ê· )
- **6ê°œì›”**: 60% (ìš°ìˆ˜)
- **12ê°œì›”**: 65% (ìµœê³  ìˆ˜ì¤€)

---

**ë‹¤ìŒ ë‹¨ê³„**: ë°±í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ êµ¬ì¶• ë° Baseline ì¸¡ì • ì°©ìˆ˜

**ì‘ì„±ì**: Claude Code
**ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-20
